{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate, Dot, Merge, Lambda, multiply\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import cross_validation, metrics\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def load_task(data_dir, task_id, only_supporting=False):\n",
    "    '''Load the nth task. There are 20 tasks in total.\n",
    "\n",
    "    Returns a tuple containing the training and testing data for the task.\n",
    "    '''\n",
    "    assert task_id > 0 and task_id < 21\n",
    "\n",
    "    files = os.listdir(data_dir)\n",
    "    files = [os.path.join(data_dir, f) for f in files]\n",
    "    s = 'qa{}_'.format(task_id)\n",
    "    train_file = [f for f in files if s in f and 'train' in f][0]\n",
    "    test_file = [f for f in files if s in f and 'test' in f][0]\n",
    "    train_data = get_stories(train_file, only_supporting)\n",
    "    test_data = get_stories(test_file, only_supporting)\n",
    "    return train_data, test_data\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbI tasks format\n",
    "    If only_supporting is true, only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = str.lower(line)\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line: # question\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            #a = tokenize(a)\n",
    "            # answer is one vocab word even if it's actually multiple words\n",
    "            a = [a]\n",
    "            substory = None\n",
    "\n",
    "            # remove question marks\n",
    "            if q[-1] == \"?\":\n",
    "                q = q[:-1]\n",
    "\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else: # regular sentence\n",
    "            # remove periods\n",
    "            sent = tokenize(line)\n",
    "            if sent[-1] == \".\":\n",
    "                sent = sent[:-1]\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False):\n",
    "    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
    "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    with open(f) as f:\n",
    "        return parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    \n",
    "def vectorize_data(data, word_idx, sentence_size, memory_size):\n",
    "    \"\"\"\n",
    "    Vectorize stories and queries.\n",
    "\n",
    "    If a sentence length < sentence_size, the sentence will be padded with 0's.\n",
    "\n",
    "    If a story length < memory_size, the story will be padded with empty memories.\n",
    "    Empty memories are 1-D arrays of length sentence_size filled with 0's.\n",
    "\n",
    "    The answer array is returned as a one-hot encoding.\n",
    "    \"\"\"\n",
    "    S = []\n",
    "    Q = []\n",
    "    A = []\n",
    "    for story, query, answer in data:\n",
    "        ss = []\n",
    "        for i, sentence in enumerate(story, 1):\n",
    "            ls = max(0, sentence_size - len(sentence))\n",
    "            ss.append([word_idx[w] for w in sentence] + [0] * ls)\n",
    "\n",
    "        # take only the most recent sentences that fit in memory\n",
    "        ss = ss[::-1][:memory_size][::-1]\n",
    "\n",
    "        # pad to memory_size\n",
    "        lm = max(0, memory_size - len(ss))\n",
    "        for _ in range(lm):\n",
    "            ss.append([0] * sentence_size)\n",
    "\n",
    "        lq = max(0, sentence_size - len(query))\n",
    "        q = [word_idx[w] for w in query] + [0] * lq\n",
    "\n",
    "#        y = np.zeros(len(word_idx) + 1) # 0 is reserved for nil word\n",
    "#        for a in answer:\n",
    "#            y[word_idx[a]] = 1\n",
    "\n",
    "        S.append(ss)\n",
    "        Q.append(q)\n",
    "        A.append(word_idx[answer[0]])\n",
    "    return np.array(S), np.array(Q), np.array(A)\n",
    "\n",
    "path_1k = '/home/ro/dataset/tasks_1-20_v1-2/en/'\n",
    "path_10k = '/home/ro/dataset/tasks_1-20_v1-2/en-10k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# task data\n",
    "def prepossessing(task_id):\n",
    "    train, test = load_task(path_1k, task_id)\n",
    "    data = train + test\n",
    "\n",
    "    vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q + a) for s, q, a in data)))\n",
    "    word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "    max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "    mean_story_size = int(np.mean([ len(s) for s, _, _ in data ]))\n",
    "    sentence_size = max(map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "    query_size = max(map(len, (q for _, q, _ in data)))\n",
    "    memory_size = min(320, max_story_size)\n",
    "\n",
    "    vocab_size = len(word_idx) + 1 # +1 for nil word\n",
    "    sentence_size = max(query_size, sentence_size) # for the position\n",
    "\n",
    "\n",
    "\n",
    "    # train/validation/test sets\n",
    "    trainS, trainQ, trainA = vectorize_data(train, word_idx, sentence_size, memory_size)\n",
    "    testS, testQ, testA = vectorize_data(test, word_idx, sentence_size, memory_size)\n",
    "\n",
    "#    print('trainS example', trainS[0])\n",
    "#    print('trainQ example', trainQ[0])\n",
    "#    print('trainA example', trainA[0])\n",
    "\n",
    "    print(\"task_id\", task_id)\n",
    "    print(\"Vocab size\", vocab_size)\n",
    "    print(\"Longest sentence length\", sentence_size)\n",
    "    print(\"Longest story length\", max_story_size)\n",
    "    print(\"Average story length\", mean_story_size)\n",
    "    print(\"Query size\", query_size)\n",
    "    \n",
    "    print('-')\n",
    "    print('trainS shape:', trainS.shape)\n",
    "    print('testS shape:', testS.shape)\n",
    "    print('-')\n",
    "    print('trainQ shape:', trainQ.shape)\n",
    "    print('testQ shape:', testQ.shape)\n",
    "    print('-')\n",
    "    print('trainA shape:', trainA.shape)\n",
    "    print('testA shape:', testA.shape)\n",
    "    print('-')\n",
    "    \n",
    "    return trainS, trainQ, trainA, testS, testQ, testA, max_story_size, sentence_size, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(max_story_size, sentence_size, vocab_size, d, k_hop):\n",
    "    # placeholders\n",
    "    story = Input(shape=(max_story_size, sentence_size,))\n",
    "    question = Input(shape=(sentence_size,))\n",
    "\n",
    "    # encoders\n",
    "    emb_A = []\n",
    "    for i in range(k_hop + 1) :\n",
    "        emb_A.append(Embedding(input_dim=vocab_size, output_dim=d, embeddings_initializer='random_normal'))\n",
    "\n",
    "    m_emb = []\n",
    "    c_emb = []\n",
    "    u_emb = []\n",
    "\n",
    "    m_emb.append(emb_A[0](story))\n",
    "    m_emb[0] = Lambda(lambda x: K.sum(x, axis=2))(m_emb[0])\n",
    "\n",
    "    u_emb.append(emb_A[0](question))\n",
    "    u_emb[0] = Lambda(lambda x: K.sum(x, axis=1))(u_emb[0])\n",
    "\n",
    "\n",
    "    for i in range(k_hop) :\n",
    "        u_temp = Lambda(lambda x: K.expand_dims(x, 1))(u_emb[i])\n",
    "        u_temp = Lambda(lambda x: K.tile(x, (1, max_story_size, 1)))(u_temp)\n",
    "\n",
    "        probs = multiply([m_emb[i], u_temp])\n",
    "        probs = Lambda(lambda x: K.sum(x, axis=2))(probs)\n",
    "        probs = Activation('softmax')(probs)\n",
    "        probs_temp = Lambda(lambda x: K.expand_dims(x, 2))(probs)\n",
    "        probs_temp = Lambda(lambda x: K.tile(x, (1, 1, d)))(probs_temp)\n",
    "\n",
    "        c_emb.append(emb_A[i + 1](story))\n",
    "        c_emb[i] = Lambda(lambda x: K.sum(x, axis=2))(c_emb[i])\n",
    "\n",
    "        o_weight = multiply([c_emb[i], probs_temp])\n",
    "        o_weight = Lambda(lambda x: K.sum(x, axis=1))(o_weight)\n",
    "\n",
    "        u_emb.append(add([o_weight, u_emb[i]]))\n",
    "\n",
    "        m_emb.append(emb_A[i + 1](story))\n",
    "        m_emb[i + 1] = Lambda(lambda x: K.sum(x, axis=2))(m_emb[i + 1])\n",
    "    \n",
    "    u_temp = Lambda(lambda x: K.expand_dims(x, 1))(u_emb[-1])\n",
    "    u_temp = Lambda(lambda x: K.tile(x, (1, vocab_size, 1)))(u_temp)\n",
    "\n",
    "    answer = Lambda(lambda x: np.array(emb_A[-1].get_weights()[0]) * x)(u_temp)\n",
    "    answer = Lambda(lambda x: K.sum(x, axis=1))(answer)\n",
    "    \n",
    "    answer = Activation('softmax')(answer)\n",
    "\n",
    "    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model = Model([story, question], answer)\n",
    "    model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ro/py3env/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_id 1\n",
      "Vocab size 20\n",
      "Longest sentence length 6\n",
      "Longest story length 10\n",
      "Average story length 6\n",
      "Query size 3\n",
      "-\n",
      "trainS shape: (1000, 10, 6)\n",
      "testS shape: (1000, 10, 6)\n",
      "-\n",
      "trainQ shape: (1000, 6)\n",
      "testQ shape: (1000, 6)\n",
      "-\n",
      "trainA shape: (1000,)\n",
      "testA shape: (1000,)\n",
      "-\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_41 (InputLayer)           (None, 10, 6)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_42 (InputLayer)           (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_81 (Embedding)        multiple             260         input_41[0][0]                   \n",
      "                                                                 input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_533 (Lambda)             (None, 13)           0           embedding_81[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_534 (Lambda)             (None, 1, 13)        0           lambda_533[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_532 (Lambda)             (None, 10, 13)       0           embedding_81[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_535 (Lambda)             (None, 10, 13)       0           lambda_534[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_121 (Multiply)         (None, 10, 13)       0           lambda_532[0][0]                 \n",
      "                                                                 lambda_535[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_536 (Lambda)             (None, 10)           0           multiply_121[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 10)           0           lambda_536[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_82 (Embedding)        (None, 10, 6, 13)    260         input_41[0][0]                   \n",
      "                                                                 input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_537 (Lambda)             (None, 10, 1)        0           activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_539 (Lambda)             (None, 10, 13)       0           embedding_82[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_538 (Lambda)             (None, 10, 13)       0           lambda_537[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_122 (Multiply)         (None, 10, 13)       0           lambda_539[0][0]                 \n",
      "                                                                 lambda_538[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_540 (Lambda)             (None, 13)           0           multiply_122[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 13)           0           lambda_540[0][0]                 \n",
      "                                                                 lambda_533[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_542 (Lambda)             (None, 1, 13)        0           add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_541 (Lambda)             (None, 10, 13)       0           embedding_82[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_543 (Lambda)             (None, 10, 13)       0           lambda_542[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_123 (Multiply)         (None, 10, 13)       0           lambda_541[0][0]                 \n",
      "                                                                 lambda_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_544 (Lambda)             (None, 10)           0           multiply_123[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 10)           0           lambda_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_83 (Embedding)        (None, 10, 6, 13)    260         input_41[0][0]                   \n",
      "                                                                 input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_545 (Lambda)             (None, 10, 1)        0           activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_547 (Lambda)             (None, 10, 13)       0           embedding_83[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_546 (Lambda)             (None, 10, 13)       0           lambda_545[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_124 (Multiply)         (None, 10, 13)       0           lambda_547[0][0]                 \n",
      "                                                                 lambda_546[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_548 (Lambda)             (None, 13)           0           multiply_124[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 13)           0           lambda_548[0][0]                 \n",
      "                                                                 add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_550 (Lambda)             (None, 1, 13)        0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_549 (Lambda)             (None, 10, 13)       0           embedding_83[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_551 (Lambda)             (None, 10, 13)       0           lambda_550[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_125 (Multiply)         (None, 10, 13)       0           lambda_549[0][0]                 \n",
      "                                                                 lambda_551[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_552 (Lambda)             (None, 10)           0           multiply_125[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 10)           0           lambda_552[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_84 (Embedding)        (None, 10, 6, 13)    260         input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_553 (Lambda)             (None, 10, 1)        0           activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_555 (Lambda)             (None, 10, 13)       0           embedding_84[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_554 (Lambda)             (None, 10, 13)       0           lambda_553[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_126 (Multiply)         (None, 10, 13)       0           lambda_555[0][0]                 \n",
      "                                                                 lambda_554[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_556 (Lambda)             (None, 13)           0           multiply_126[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 13)           0           lambda_556[0][0]                 \n",
      "                                                                 add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_558 (Lambda)             (None, 1, 13)        0           add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_559 (Lambda)             (None, 20, 13)       0           lambda_558[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_560 (Lambda)             (None, 20, 13)       0           lambda_559[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 20, 13)       0           lambda_560[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,040\n",
      "Trainable params: 1,040\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "d = 13\n",
    "k_hop = 3\n",
    "i = 1\n",
    "\n",
    "trainS, trainQ, trainA, testS, testQ, testA, max_story_size, sentence_size, vocab_size = prepossessing(i)\n",
    "\n",
    "#trainA = np.expand_dims(trainA,-1)\n",
    "#testA = np.expand_dims(testA,-1)\n",
    "\n",
    "model = get_model(max_story_size, sentence_size, vocab_size, d, k_hop)\n",
    "#model.fit([trainS, trainQ], trainA,batch_size=32,epochs=100,validation_data=([testS, testQ], testA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20) :\n",
    "    print('task_id', i)\n",
    "    model[i].evaluate([testS, testQ], testA, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
