{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ro/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/ro/py3env/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout, add, dot, concatenate, Dot, Merge, Lambda, multiply\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn import cross_validation, metrics\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def load_task(data_dir, task_id, only_supporting=False):\n",
    "    '''Load the nth task. There are 20 tasks in total.\n",
    "\n",
    "    Returns a tuple containing the training and testing data for the task.\n",
    "    '''\n",
    "    assert task_id > 0 and task_id < 21\n",
    "\n",
    "    files = os.listdir(data_dir)\n",
    "    files = [os.path.join(data_dir, f) for f in files]\n",
    "    s = 'qa{}_'.format(task_id)\n",
    "    train_file = [f for f in files if s in f and 'train' in f][0]\n",
    "    test_file = [f for f in files if s in f and 'test' in f][0]\n",
    "    train_data = get_stories(train_file, only_supporting)\n",
    "    test_data = get_stories(test_file, only_supporting)\n",
    "    return train_data, test_data\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbI tasks format\n",
    "    If only_supporting is true, only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = str.lower(line)\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line: # question\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            #a = tokenize(a)\n",
    "            # answer is one vocab word even if it's actually multiple words\n",
    "            a = [a]\n",
    "            substory = None\n",
    "\n",
    "            # remove question marks\n",
    "            if q[-1] == \"?\":\n",
    "                q = q[:-1]\n",
    "\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else: # regular sentence\n",
    "            # remove periods\n",
    "            sent = tokenize(line)\n",
    "            if sent[-1] == \".\":\n",
    "                sent = sent[:-1]\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False):\n",
    "    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
    "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    with open(f) as f:\n",
    "        return parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    \n",
    "def vectorize_data(data, word_idx, sentence_size, memory_size):\n",
    "    \"\"\"\n",
    "    Vectorize stories and queries.\n",
    "\n",
    "    If a sentence length < sentence_size, the sentence will be padded with 0's.\n",
    "\n",
    "    If a story length < memory_size, the story will be padded with empty memories.\n",
    "    Empty memories are 1-D arrays of length sentence_size filled with 0's.\n",
    "\n",
    "    The answer array is returned as a one-hot encoding.\n",
    "    \"\"\"\n",
    "    S = []\n",
    "    Q = []\n",
    "    A = []\n",
    "    for story, query, answer in data:\n",
    "        ss = []\n",
    "        for i, sentence in enumerate(story, 1):\n",
    "            ls = max(0, sentence_size - len(sentence))\n",
    "            ss.append([word_idx[w] for w in sentence] + [0] * ls)\n",
    "\n",
    "        # take only the most recent sentences that fit in memory\n",
    "        ss = ss[::-1][:memory_size][::-1]\n",
    "\n",
    "        # pad to memory_size\n",
    "        lm = max(0, memory_size - len(ss))\n",
    "        for _ in range(lm):\n",
    "            ss.append([0] * sentence_size)\n",
    "\n",
    "        lq = max(0, sentence_size - len(query))\n",
    "        q = [word_idx[w] for w in query] + [0] * lq\n",
    "\n",
    "#        y = np.zeros(len(word_idx) + 1) # 0 is reserved for nil word\n",
    "#        for a in answer:\n",
    "#            y[word_idx[a]] = 1\n",
    "\n",
    "        S.append(ss)\n",
    "        Q.append(q)\n",
    "        A.append(word_idx[answer[0]])\n",
    "    return np.array(S), np.array(Q), np.array(A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_1k = '/home/ro/dataset/tasks_1-20_v1-2/en/'\n",
    "path_10k = '/home/ro/dataset/tasks_1-20_v1-2/en-10k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 20\n",
      "Longest sentence length 6\n",
      "Longest story length 10\n",
      "Average story length 6\n",
      "Query size 3\n",
      "trainS example [[11 12 16 15  2  0]\n",
      " [ 8 18 16 15  6  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "trainQ example [19  7 11  0  0  0]\n",
      "trainA example 2\n",
      "-\n",
      "trainS shape: (1000, 10, 6)\n",
      "testS shape: (1000, 10, 6)\n",
      "-\n",
      "trainQ shape: (1000, 6)\n",
      "testQ shape: (1000, 6)\n",
      "-\n",
      "trainA shape: (1000,)\n",
      "testA shape: (1000,)\n",
      "-\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ro/py3env/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "# task data\n",
    "train, test = load_task(path_1k, 1)\n",
    "data = train + test\n",
    "\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q + a) for s, q, a in data)))\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "mean_story_size = int(np.mean([ len(s) for s, _, _ in data ]))\n",
    "sentence_size = max(map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "query_size = max(map(len, (q for _, q, _ in data)))\n",
    "memory_size = min(320, max_story_size)\n",
    "\n",
    "vocab_size = len(word_idx) + 1 # +1 for nil word\n",
    "sentence_size = max(query_size, sentence_size) # for the position\n",
    "\n",
    "print(\"Vocab size\", vocab_size)\n",
    "print(\"Longest sentence length\", sentence_size)\n",
    "print(\"Longest story length\", max_story_size)\n",
    "print(\"Average story length\", mean_story_size)\n",
    "print(\"Query size\", query_size)\n",
    "\n",
    "# train/validation/test sets\n",
    "trainS, trainQ, trainA = vectorize_data(train, word_idx, sentence_size, memory_size)\n",
    "testS, testQ, testA = vectorize_data(test, word_idx, sentence_size, memory_size)\n",
    "\n",
    "print('trainS example', trainS[0])\n",
    "print('trainQ example', trainQ[0])\n",
    "print('trainA example', trainA[0])\n",
    "\n",
    "print('-')\n",
    "print('trainS shape:', trainS.shape)\n",
    "print('testS shape:', testS.shape)\n",
    "print('-')\n",
    "print('trainQ shape:', trainQ.shape)\n",
    "print('testQ shape:', testQ.shape)\n",
    "print('-')\n",
    "print('trainA shape:', trainA.shape)\n",
    "print('testA shape:', testA.shape)\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           (None, 10, 6)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           (None, 6)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_61 (Embedding)        multiple             400         input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_392 (Lambda)             (None, 20)           0           embedding_61[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_393 (Lambda)             (None, 1, 20)        0           lambda_392[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_391 (Lambda)             (None, 10, 20)       0           embedding_61[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_394 (Lambda)             (None, 10, 20)       0           lambda_393[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_91 (Multiply)          (None, 10, 20)       0           lambda_391[0][0]                 \n",
      "                                                                 lambda_394[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_395 (Lambda)             (None, 10)           0           multiply_91[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 10)           0           lambda_395[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_62 (Embedding)        (None, 10, 6, 20)    400         input_31[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_396 (Lambda)             (None, 10, 1)        0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_398 (Lambda)             (None, 10, 20)       0           embedding_62[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_397 (Lambda)             (None, 10, 20)       0           lambda_396[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_92 (Multiply)          (None, 10, 20)       0           lambda_398[0][0]                 \n",
      "                                                                 lambda_397[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_399 (Lambda)             (None, 20)           0           multiply_92[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 20)           0           lambda_399[0][0]                 \n",
      "                                                                 lambda_392[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_401 (Lambda)             (None, 1, 20)        0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_400 (Lambda)             (None, 10, 20)       0           embedding_62[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_402 (Lambda)             (None, 10, 20)       0           lambda_401[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_93 (Multiply)          (None, 10, 20)       0           lambda_400[0][0]                 \n",
      "                                                                 lambda_402[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_403 (Lambda)             (None, 10)           0           multiply_93[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 10)           0           lambda_403[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_63 (Embedding)        (None, 10, 6, 20)    400         input_31[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_404 (Lambda)             (None, 10, 1)        0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_406 (Lambda)             (None, 10, 20)       0           embedding_63[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_405 (Lambda)             (None, 10, 20)       0           lambda_404[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_94 (Multiply)          (None, 10, 20)       0           lambda_406[0][0]                 \n",
      "                                                                 lambda_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_407 (Lambda)             (None, 20)           0           multiply_94[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 20)           0           lambda_407[0][0]                 \n",
      "                                                                 add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_409 (Lambda)             (None, 1, 20)        0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_408 (Lambda)             (None, 10, 20)       0           embedding_63[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_410 (Lambda)             (None, 10, 20)       0           lambda_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_95 (Multiply)          (None, 10, 20)       0           lambda_408[0][0]                 \n",
      "                                                                 lambda_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_411 (Lambda)             (None, 10)           0           multiply_95[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 10)           0           lambda_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_64 (Embedding)        (None, 10, 6, 20)    400         input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_412 (Lambda)             (None, 10, 1)        0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_414 (Lambda)             (None, 10, 20)       0           embedding_64[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_413 (Lambda)             (None, 10, 20)       0           lambda_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_96 (Multiply)          (None, 10, 20)       0           lambda_414[0][0]                 \n",
      "                                                                 lambda_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_415 (Lambda)             (None, 20)           0           multiply_96[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 20)           0           lambda_415[0][0]                 \n",
      "                                                                 add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 20)           420         add_48[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,020\n",
      "Trainable params: 2,020\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "d = 20\n",
    "k_hop = 3\n",
    "\n",
    "# placeholders\n",
    "story = Input(shape=(max_story_size, sentence_size,))\n",
    "question = Input(shape=(sentence_size,))\n",
    "\n",
    "# encoders\n",
    "emb_A = []\n",
    "for i in range(k_hop + 1) :\n",
    "    emb_A.append(Embedding(input_dim=vocab_size, output_dim=d, embeddings_initializer='random_normal'))\n",
    "\n",
    "m_emb = []\n",
    "c_emb = []\n",
    "u_emb = []\n",
    "\n",
    "m_emb.append(emb_A[0](story))\n",
    "m_emb[0] = Lambda(lambda x: K.sum(x, axis=2))(m_emb[0])\n",
    "\n",
    "u_emb.append(emb_A[0](question))\n",
    "u_emb[0] = Lambda(lambda x: K.sum(x, axis=1))(u_emb[0])\n",
    "\n",
    "for i in range(k_hop) :\n",
    "    u_temp = Lambda(lambda x: K.expand_dims(x, 1))(u_emb[i])\n",
    "    u_temp = Lambda(lambda x: K.tile(x, (1, max_story_size, 1)))(u_temp)\n",
    "\n",
    "    probs = multiply([m_emb[i], u_temp])\n",
    "    probs = Lambda(lambda x: K.sum(x, axis=2))(probs)\n",
    "    probs = Activation('softmax')(probs)\n",
    "    probs_temp = Lambda(lambda x: K.expand_dims(x, 2))(probs)\n",
    "    probs_temp = Lambda(lambda x: K.tile(x, (1, 1, d)))(probs_temp)\n",
    "    \n",
    "    c_emb.append(emb_A[i + 1](story))\n",
    "    c_emb[i] = Lambda(lambda x: K.sum(x, axis=2))(c_emb[i])\n",
    "    \n",
    "    o_weight = multiply([c_emb[i], probs_temp])\n",
    "    o_weight = Lambda(lambda x: K.sum(x, axis=1))(o_weight)\n",
    "\n",
    "    u_emb.append(add([o_weight, u_emb[i]]))\n",
    "\n",
    "    m_emb.append(emb_A[i + 1](story))\n",
    "    m_emb[i + 1] = Lambda(lambda x: K.sum(x, axis=2))(m_emb[i + 1])\n",
    "\n",
    "answer = Dense(vocab_size, activation='softmax', kernel_initializer='random_normal')(u_emb[-1])\n",
    "#answer = Activation('softmax')(answer)\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model = Model([story, question], answer)\n",
    "model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 2.0915 - acc: 0.1470 - val_loss: 1.8735 - val_acc: 0.1820\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 0s 451us/step - loss: 1.8467 - acc: 0.1590 - val_loss: 1.8420 - val_acc: 0.1710\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 0s 444us/step - loss: 1.8328 - acc: 0.1690 - val_loss: 1.8185 - val_acc: 0.1870\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 0s 440us/step - loss: 1.8193 - acc: 0.1670 - val_loss: 1.8387 - val_acc: 0.1710\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 0s 463us/step - loss: 1.8315 - acc: 0.1770 - val_loss: 1.8463 - val_acc: 0.1750\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 0s 450us/step - loss: 1.8169 - acc: 0.1840 - val_loss: 1.8048 - val_acc: 0.1820\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 0s 452us/step - loss: 1.8141 - acc: 0.1990 - val_loss: 1.7984 - val_acc: 0.2060\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 0s 424us/step - loss: 1.7819 - acc: 0.2080 - val_loss: 1.7682 - val_acc: 0.2210\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 0s 451us/step - loss: 1.7709 - acc: 0.2080 - val_loss: 1.7530 - val_acc: 0.2330\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 0s 437us/step - loss: 1.7409 - acc: 0.2630 - val_loss: 1.7456 - val_acc: 0.2290\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 0s 438us/step - loss: 1.7300 - acc: 0.2570 - val_loss: 1.7238 - val_acc: 0.2570\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 0s 434us/step - loss: 1.6922 - acc: 0.2950 - val_loss: 1.7150 - val_acc: 0.2840\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 0s 442us/step - loss: 1.6701 - acc: 0.3020 - val_loss: 1.7594 - val_acc: 0.2620\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 0s 468us/step - loss: 1.6447 - acc: 0.3290 - val_loss: 1.6818 - val_acc: 0.2810\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 0s 476us/step - loss: 1.6240 - acc: 0.3320 - val_loss: 1.7026 - val_acc: 0.3080\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 0s 417us/step - loss: 1.5527 - acc: 0.3830 - val_loss: 1.7297 - val_acc: 0.2990\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 0s 457us/step - loss: 1.5336 - acc: 0.3750 - val_loss: 1.5260 - val_acc: 0.3920\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 521us/step - loss: 1.4679 - acc: 0.4400 - val_loss: 1.5427 - val_acc: 0.3900\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 0s 452us/step - loss: 1.3665 - acc: 0.4760 - val_loss: 1.3769 - val_acc: 0.4760\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 0s 445us/step - loss: 1.1727 - acc: 0.5640 - val_loss: 1.3309 - val_acc: 0.5120\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 516us/step - loss: 1.0189 - acc: 0.6210 - val_loss: 1.0870 - val_acc: 0.5760\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 0s 467us/step - loss: 0.9117 - acc: 0.6450 - val_loss: 1.0220 - val_acc: 0.6360\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 0s 477us/step - loss: 0.8067 - acc: 0.7000 - val_loss: 0.8851 - val_acc: 0.6270\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 531us/step - loss: 0.8079 - acc: 0.6690 - val_loss: 0.8860 - val_acc: 0.6260\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 544us/step - loss: 0.8045 - acc: 0.6770 - val_loss: 0.9872 - val_acc: 0.6260\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 506us/step - loss: 0.7712 - acc: 0.6980 - val_loss: 1.0927 - val_acc: 0.6130\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 0s 481us/step - loss: 0.7412 - acc: 0.6990 - val_loss: 0.8873 - val_acc: 0.6310\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 524us/step - loss: 0.7142 - acc: 0.7050 - val_loss: 0.9657 - val_acc: 0.6450\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 0s 500us/step - loss: 0.7123 - acc: 0.6960 - val_loss: 0.8158 - val_acc: 0.6520\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 528us/step - loss: 0.6937 - acc: 0.7250 - val_loss: 0.8846 - val_acc: 0.6340\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 0s 471us/step - loss: 0.6889 - acc: 0.7170 - val_loss: 0.8990 - val_acc: 0.6430\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 0s 496us/step - loss: 0.6767 - acc: 0.7250 - val_loss: 1.0041 - val_acc: 0.6420\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 0s 477us/step - loss: 0.6816 - acc: 0.7320 - val_loss: 0.8342 - val_acc: 0.6470\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 503us/step - loss: 0.6763 - acc: 0.7180 - val_loss: 0.8249 - val_acc: 0.6560\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 514us/step - loss: 0.6632 - acc: 0.7160 - val_loss: 0.9651 - val_acc: 0.6240\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 0s 476us/step - loss: 0.7688 - acc: 0.6930 - val_loss: 1.0112 - val_acc: 0.6410\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 0s 445us/step - loss: 0.6935 - acc: 0.7070 - val_loss: 0.8623 - val_acc: 0.6350\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 0s 445us/step - loss: 0.6496 - acc: 0.7260 - val_loss: 0.8898 - val_acc: 0.6580\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 0s 491us/step - loss: 0.6457 - acc: 0.7130 - val_loss: 0.9246 - val_acc: 0.6390\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 0s 487us/step - loss: 0.6522 - acc: 0.7320 - val_loss: 0.8695 - val_acc: 0.6320\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 0s 438us/step - loss: 0.6884 - acc: 0.7200 - val_loss: 0.8859 - val_acc: 0.6510\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 0s 454us/step - loss: 0.6416 - acc: 0.7330 - val_loss: 0.8858 - val_acc: 0.6390\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 0s 467us/step - loss: 0.6453 - acc: 0.7210 - val_loss: 0.9474 - val_acc: 0.6420\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 0s 467us/step - loss: 0.6248 - acc: 0.7420 - val_loss: 0.8292 - val_acc: 0.6600\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 0s 475us/step - loss: 0.6308 - acc: 0.7490 - val_loss: 0.9330 - val_acc: 0.6500\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 1s 516us/step - loss: 0.5967 - acc: 0.7500 - val_loss: 0.9174 - val_acc: 0.6360\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 1s 502us/step - loss: 0.6462 - acc: 0.7230 - val_loss: 0.9030 - val_acc: 0.6580\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 0s 499us/step - loss: 0.6246 - acc: 0.7560 - val_loss: 0.9122 - val_acc: 0.6330\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 0s 464us/step - loss: 0.6353 - acc: 0.7370 - val_loss: 0.9977 - val_acc: 0.6230\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 0s 444us/step - loss: 0.6475 - acc: 0.7260 - val_loss: 0.8874 - val_acc: 0.6320\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 0s 475us/step - loss: 0.5968 - acc: 0.7350 - val_loss: 0.9926 - val_acc: 0.6400\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 0s 456us/step - loss: 0.6351 - acc: 0.7280 - val_loss: 0.9012 - val_acc: 0.6410\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 0s 438us/step - loss: 0.6501 - acc: 0.7200 - val_loss: 0.8923 - val_acc: 0.6390\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 0s 456us/step - loss: 0.6515 - acc: 0.7370 - val_loss: 0.9587 - val_acc: 0.6260\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 0s 481us/step - loss: 0.6535 - acc: 0.7250 - val_loss: 1.0815 - val_acc: 0.5930\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 1s 506us/step - loss: 0.6437 - acc: 0.7100 - val_loss: 0.8677 - val_acc: 0.6470\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 1s 568us/step - loss: 0.6345 - acc: 0.7330 - val_loss: 0.9312 - val_acc: 0.6390\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 0s 487us/step - loss: 0.6102 - acc: 0.7370 - val_loss: 0.8832 - val_acc: 0.6470\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 1s 549us/step - loss: 0.6266 - acc: 0.7400 - val_loss: 1.0057 - val_acc: 0.6310\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 1s 545us/step - loss: 0.6467 - acc: 0.7270 - val_loss: 0.9468 - val_acc: 0.6370\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 1s 520us/step - loss: 0.6303 - acc: 0.7340 - val_loss: 0.9330 - val_acc: 0.6370\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 1s 528us/step - loss: 0.6136 - acc: 0.7460 - val_loss: 0.9655 - val_acc: 0.6260\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 1s 530us/step - loss: 0.5940 - acc: 0.7460 - val_loss: 0.9389 - val_acc: 0.6220\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 0s 471us/step - loss: 0.6111 - acc: 0.7490 - val_loss: 0.9628 - val_acc: 0.6250\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 0s 468us/step - loss: 0.6100 - acc: 0.7480 - val_loss: 0.9630 - val_acc: 0.6480\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 0s 486us/step - loss: 0.6154 - acc: 0.7460 - val_loss: 0.9078 - val_acc: 0.6320\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 0s 476us/step - loss: 0.6160 - acc: 0.7480 - val_loss: 0.9764 - val_acc: 0.6400\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 1s 521us/step - loss: 0.5923 - acc: 0.7510 - val_loss: 1.0234 - val_acc: 0.6240\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 1s 517us/step - loss: 0.6414 - acc: 0.7490 - val_loss: 0.9931 - val_acc: 0.6440\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 1s 529us/step - loss: 0.6457 - acc: 0.7390 - val_loss: 0.9336 - val_acc: 0.6330\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 1s 523us/step - loss: 0.5835 - acc: 0.7610 - val_loss: 0.9264 - val_acc: 0.6240\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 1s 532us/step - loss: 0.5739 - acc: 0.7690 - val_loss: 0.9030 - val_acc: 0.6320\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 0s 455us/step - loss: 0.6066 - acc: 0.7600 - val_loss: 1.0165 - val_acc: 0.6300\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 0s 453us/step - loss: 0.7234 - acc: 0.7270 - val_loss: 1.1399 - val_acc: 0.6140\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 0s 493us/step - loss: 0.7838 - acc: 0.7180 - val_loss: 1.0117 - val_acc: 0.6450\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 1s 512us/step - loss: 0.6888 - acc: 0.7200 - val_loss: 0.9270 - val_acc: 0.6310\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 1s 510us/step - loss: 0.6448 - acc: 0.7270 - val_loss: 0.9349 - val_acc: 0.6350\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 0s 459us/step - loss: 0.5915 - acc: 0.7470 - val_loss: 0.8884 - val_acc: 0.6500\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 0s 495us/step - loss: 0.6043 - acc: 0.7460 - val_loss: 0.9549 - val_acc: 0.6530\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 0s 496us/step - loss: 0.6022 - acc: 0.7520 - val_loss: 0.9136 - val_acc: 0.6320\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 1s 520us/step - loss: 0.5811 - acc: 0.7540 - val_loss: 0.8997 - val_acc: 0.6420\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 1s 504us/step - loss: 0.5714 - acc: 0.7610 - val_loss: 0.9779 - val_acc: 0.6350\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 0s 483us/step - loss: 0.5890 - acc: 0.7440 - val_loss: 1.0117 - val_acc: 0.6190\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 0s 480us/step - loss: 0.5919 - acc: 0.7460 - val_loss: 1.0101 - val_acc: 0.6300\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 0s 475us/step - loss: 0.5871 - acc: 0.7680 - val_loss: 1.0338 - val_acc: 0.6170\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 1s 520us/step - loss: 0.5833 - acc: 0.7660 - val_loss: 1.0719 - val_acc: 0.6160\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 1s 500us/step - loss: 0.5899 - acc: 0.7550 - val_loss: 0.9226 - val_acc: 0.6210\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 0s 487us/step - loss: 0.5650 - acc: 0.7530 - val_loss: 0.9973 - val_acc: 0.6420\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 0s 454us/step - loss: 0.5996 - acc: 0.7690 - val_loss: 1.0338 - val_acc: 0.6320\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 1s 502us/step - loss: 0.6034 - acc: 0.7580 - val_loss: 0.9832 - val_acc: 0.6340\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 0s 480us/step - loss: 0.6109 - acc: 0.7540 - val_loss: 1.1878 - val_acc: 0.5830\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 0s 456us/step - loss: 0.6111 - acc: 0.7380 - val_loss: 0.9864 - val_acc: 0.6500\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 1s 504us/step - loss: 0.6245 - acc: 0.7360 - val_loss: 0.9969 - val_acc: 0.6200\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 0s 479us/step - loss: 0.5746 - acc: 0.7580 - val_loss: 1.0174 - val_acc: 0.6110\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 0s 449us/step - loss: 0.5910 - acc: 0.7320 - val_loss: 0.9840 - val_acc: 0.6360\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 1s 508us/step - loss: 0.6018 - acc: 0.7730 - val_loss: 0.9678 - val_acc: 0.6180\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 1s 521us/step - loss: 0.5991 - acc: 0.7590 - val_loss: 0.9507 - val_acc: 0.6270\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 0s 495us/step - loss: 0.6004 - acc: 0.7520 - val_loss: 1.0550 - val_acc: 0.6210\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 1s 518us/step - loss: 0.6412 - acc: 0.7400 - val_loss: 1.0296 - val_acc: 0.6220\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 1s 505us/step - loss: 0.5634 - acc: 0.7660 - val_loss: 1.0210 - val_acc: 0.6260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c05f7f2e8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model.fit([trainS, trainQ], trainA,\n",
    "          batch_size=16,\n",
    "          epochs=100,\n",
    "          validation_data=([testS, testQ], testA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 86us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0210048589706422, 0.626]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([testS, testQ], testA, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "d = 3\n",
    "vocab_size = 4\n",
    "max_story_size = 2\n",
    "sentence_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_45 (InputLayer)        (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_72 (Embedding)     (None, 5, 3)              12        \n",
      "_________________________________________________________________\n",
      "lambda_158 (Lambda)          (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# placeholders\n",
    "story = Input(shape=(max_story_size, sentence_size,))\n",
    "question = Input(shape=(sentence_size,))\n",
    "\n",
    "# encoders\n",
    "shared_memory_a = Embedding(input_dim=vocab_size, output_dim=d, embeddings_initializer='ones')\n",
    "shared_memory_b = Embedding(input_dim=vocab_size, output_dim=d, embeddings_initializer='ones')\n",
    "shared_memory_c = Embedding(input_dim=vocab_size, output_dim=d)\n",
    "\n",
    "memory_m1 = shared_memory_a(story)\n",
    "memory_m1 = Lambda(lambda x: K.sum(x, axis=2))(memory_m1)\n",
    "memory_c1 = shared_memory_c(story)\n",
    "memory_c1 = Lambda(lambda x: K.sum(x, axis=2))(memory_c1)\n",
    "u1 = shared_memory_b(question)\n",
    "u1 = Lambda(lambda x: K.sum(x, axis=1))(u1)\n",
    "\n",
    "model_1 = Model(question, u1)\n",
    "model_1.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 5)\n",
      "(1, 5)\n",
      "(1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5., 5., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.array([[[1,1,2,0,3],[3,2,2,1,3]]])\n",
    "q = np.array([[1,1,2,0,3]])\n",
    "a = np.array([1])\n",
    "print(q.shape)\n",
    "res = model_1.predict(q)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_65 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_173 (Lambda)             (None, 1, 3)         0           input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_66 (InputLayer)           (None, 2, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_174 (Lambda)             (None, 2, 3)         0           lambda_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_40 (Multiply)          (None, 2, 3)         0           input_66[0][0]                   \n",
      "                                                                 lambda_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_175 (Lambda)             (None, 2)            0           multiply_40[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 2)            0           lambda_175[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "u1 = Input(shape=(d,))\n",
    "memory_m1 = Input(shape=(max_story_size, d,))\n",
    "\n",
    "u1_temp = Lambda(lambda x: K.expand_dims(x, 1))(u1)\n",
    "u1_temp = Lambda(lambda x: K.tile(x, (1, max_story_size, 1)))(u1_temp)\n",
    "probs = multiply([memory_m1, u1_temp])\n",
    "probs = Lambda(lambda x: K.sum(x, axis=2))(probs)\n",
    "probs = Activation('softmax')(probs)\n",
    "model_2 = Model([memory_m1, u1], probs)\n",
    "model_2.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.8795287e-12, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = np.array([[[1,2,3],[4,5,6]]])\n",
    "u1 = np.array([[3,2,4]])\n",
    "a = np.array([1])\n",
    "print(u1.shape)\n",
    "res = model_2.predict([m1, u1])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_72 (InputLayer)           (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_182 (Lambda)             (None, 2, 1)         0           input_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_73 (InputLayer)           (None, 2, 3)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_183 (Lambda)             (None, 2, 3)         0           lambda_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_43 (Multiply)          (None, 2, 3)         0           input_73[0][0]                   \n",
      "                                                                 lambda_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_184 (Lambda)             (None, 3)            0           multiply_43[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "probs = Input(shape=(max_story_size,))\n",
    "memory_c1 = Input(shape=(max_story_size, d,))\n",
    "\n",
    "probs_temp = Lambda(lambda x: K.expand_dims(x, 2))(probs)\n",
    "probs_temp = Lambda(lambda x: K.tile(x, (1, 1, d)))(probs_temp)\n",
    "\n",
    "o_weight = multiply([memory_c1, probs_temp])\n",
    "o_weight = Lambda(lambda x: K.sum(x, axis=1))(o_weight)\n",
    "\n",
    "model_2 = Model([memory_c1, probs], o_weight)\n",
    "model_2.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.4, 4.4, 5.4]], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = np.array([[[1,2,3],[4,5,6]]])\n",
    "p = np.array([[0.2,0.8]])\n",
    "a = np.array([1])\n",
    "print(u1.shape)\n",
    "res = model_2.predict([c1, p])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_86 (InputLayer)        (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "lambda_193 (Lambda)          (None, 1, 3)              0         \n",
      "_________________________________________________________________\n",
      "lambda_194 (Lambda)          (None, 4, 3)              0         \n",
      "_________________________________________________________________\n",
      "lambda_195 (Lambda)          (None, 4, 3)              0         \n",
      "_________________________________________________________________\n",
      "lambda_196 (Lambda)          (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "u = Input(shape=(d,))\n",
    "\n",
    "u_temp = Lambda(lambda x: K.expand_dims(x, 1))(u)\n",
    "u_temp = Lambda(lambda x: K.tile(x, (1, vocab_size, 1)))(u_temp)\n",
    "answer = Lambda(lambda x: np.array(shared_memory_a.get_weights()[0]) * x)(u_temp)\n",
    "answer = Lambda(lambda x: K.sum(x, axis=2))(answer)\n",
    "#answer = Activation('softmax')(answer)\n",
    "\n",
    "model = Model(u, answer)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3863 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3863 - acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3863 - acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3863 - acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3863 - acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3863 - acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3863 - acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3863 - acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3863 - acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.3863 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[36., 13., 45.,  9.]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1 = np.array([[4,5,6]])\n",
    "u1 = np.array([[3,2,4]])\n",
    "a = np.array([1])\n",
    "print(u1.shape)\n",
    "res = model.fit(u1, a,epochs = 10)\n",
    "res = model.predict(u1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8., 3., 3.],\n",
       "       [1., 1., 3.],\n",
       "       [5., 5., 5.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = shared_memory_a.get_weights()[0]\n",
    "w[2] = 5\n",
    "w[0] = 3\n",
    "w[0][0] = 8\n",
    "w[1][2] = 3\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_memory_a.set_weights([w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model_single.png', show_shapes = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
